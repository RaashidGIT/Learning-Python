{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69465ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc08406",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.tail()\n",
    "data.columns\n",
    "print('lenght of data is', len(data))\n",
    "data.shape\n",
    "data.info()\n",
    "data.dtypes\n",
    "\n",
    "np.sum(data.isnull().any(axis=1))\n",
    "\n",
    "print('Count of columns in the data is:  ', len(data.columns))\n",
    "print('Count of rows in the data is:  ', len(data))\n",
    "\n",
    "data=data[['text','label']]\n",
    "data['label'][data['label']==4]=1\n",
    "\n",
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]\n",
    "\n",
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]\n",
    "\n",
    "data = pd.concat([data_pos, data_neg])\n",
    "\n",
    "data['text']=data['text'].str.lower()\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['text'].tail()\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "data['text'].tail()\n",
    "\n",
    "def cleaning_email(data):\n",
    "    return re.sub('@[^\\s]+', ' ', data)\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
    "data['text'].tail()\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
    "data['text'].tail()\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
    "data['text'].tail()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "data['text'].head()\n",
    "\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: stemming_on_text(x))\n",
    "\n",
    "data['text'].head()\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.text\n",
    "y=data.label\n",
    "\n",
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "sequences_matrix.shape\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "    layer = LSTM(64)(layer) #step3\n",
    "    layer = Dense(256,name='FC1')(layer) #step4\n",
    "    layer = Activation('relu')(layer) # step5\n",
    "    layer = Dropout(0.5)(layer) # step6\n",
    "    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
    "    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
    "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
    "    return model #function returning the value when we call it\n",
    "\n",
    "model = tensorflow_based_model() # here we are calling the function of created model\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  \n",
    "\n",
    "history=model.fit(X_train,Y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
    "print('Training finished !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here\n",
    "\n",
    "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below\n",
    "\n",
    "y_pred = model.predict(X_test) #getting predictions on the trained model\n",
    "y_pred = (y_pred > 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a500f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print(\"confusion matrix\")\n",
    "print('\\n')\n",
    "CR=confusion_matrix(Y_test, y_pred)\n",
    "print(CR)\n",
    "print('\\n')\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC CURVE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
